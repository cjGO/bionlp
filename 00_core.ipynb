{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bionlp\n",
    "\n",
    "> useful fxns for DL models with bio seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionlp_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gen_dna_vocab(frameshift=True, kmer=1, letters='ATCGN'):\n",
    "    \"\"\" Makes a dictionary for mapping a raw sequence\n",
    "    to one hot encoding\n",
    "    \"\"\"\n",
    "    token_list  = []\n",
    "    if frameshift:\n",
    "        letters = 'X' + letters\n",
    "    for i in [''.join(c) for c in product(letters, repeat=kmer)]:\n",
    "        if (i[0] == 'X') and (i[kmer-1] == 'X'):\n",
    "            pass\n",
    "        else:\n",
    "            token_list.append(i)\n",
    "\n",
    "    mapping = {}\n",
    "    idx = 1\n",
    "    for letter in token_list:\n",
    "        if letter not in mapping:\n",
    "            mapping[letter] = idx\n",
    "            idx+=1        \n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pred_vs_truth(model,dataloader):\n",
    "    \"\"\"\n",
    "    takes a trained model and runs predictions over the\n",
    "    given dataloader.\n",
    "    \n",
    "    plots hist of the ground truths vs predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    truths = []\n",
    "    preds =  []\n",
    "    for i in range(len(dataloader)):\n",
    "        batch = next(iter(dataloader))\n",
    "        preds.append(model.predict(batch[0]))\n",
    "        truths.append(batch[1])\n",
    "\n",
    "    array_pred = []\n",
    "    for i in preds:\n",
    "        for x in i:\n",
    "            array_pred.append(x)\n",
    "\n",
    "    flat_preds = np.array(array_pred).flatten()\n",
    "\n",
    "    truth_pred = []\n",
    "    for i in truths:\n",
    "        for x in i:\n",
    "            truth_pred.append(np.array(x))\n",
    "    flat_truth = np.array(truth_pred)\n",
    "\n",
    "    figure = plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.hist(flat_truth, bins=20, color = 'blue', label='Truths',alpha=.5, range=(flat_truth.min()*-1.5,flat_truth.max()*1.5))\n",
    "    plt.hist(flat_preds, bins=20, color = 'red', label='Preds', alpha=.5)\n",
    "    plt.legend(['Truth','Pred'])\n",
    "    \n",
    "    \n",
    "    r2 = round(np.corrcoef(flat_truth,flat_preds)[0][1],2)\n",
    "    \n",
    "    m, b = np. polyfit(flat_truth, flat_preds, 1)# m = slope, b = intercept.\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(flat_truth,flat_preds)\n",
    "    plt.plot(flat_truth, m*flat_truth + b)\n",
    "    plt.xlabel('truth values')\n",
    "    plt.ylabel('pred values')\n",
    "    plt.text(flat_truth.min(),flat_truth.max()*1.3, f'R2 = {str(r2)}', color='red',fontweight='heavy')\n",
    "    plt.xlim(flat_truth.min()*-1.5,flat_truth.max()*1.5)\n",
    "    plt.ylim(flat_truth.min()*-1.5,flat_truth.max()*1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prep_seqs(seq, max_length=10):\n",
    "    \"\"\"\n",
    "    takes in a raw seq 'ATTATA' -> one hot encodes to specified max_len\n",
    "      \"\"\"\n",
    "    dna_dict = gen_dna_vocab()\n",
    "    x = [dna_dict[x] for x in seq]\n",
    "    N = max_length - len(x)\n",
    "    x = np.pad(x, (0, N), 'constant')\n",
    "    x = F.one_hot(torch.tensor(x),num_classes=6)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DnaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    takes in dataframe with ['Seqs'] and ['Exp']\n",
    "    \n",
    "    returns pytorch dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.max_length = self.df.Seqs.map(len).max()\n",
    "        self.df.Exp.astype(float)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        seq = self.df['Seqs'][idx]\n",
    "        seq = prep_seqs(seq,self.max_length)\n",
    "        seq = seq.swapaxes(0,1)\n",
    "        seq = seq.float()\n",
    "\n",
    "        target = self.df['Exp'][idx]\n",
    "        target = torch.tensor(target).float()\n",
    "        #target = torch.tensor(target)\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        #MINE\n",
    "        self.images = []\n",
    "        \n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(folder, name, suffix))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                \n",
    "   \n",
    "                \n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_Seqs_Exp_Dataset(num_seq, low, high):\n",
    "    \"\"\"\n",
    "    generates a DNA dataset with Expression(or whatever else) values\n",
    "    nucleotides/expressions will be drawn from 2 distinct distributions\n",
    "    \n",
    "    input : num_seq -> int: number of sequences\n",
    "            length_seq -> tuple(min/max length of sequences)\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_sequences = []\n",
    "    dna_dict = gen_dna_vocab()\n",
    "    \n",
    "    \n",
    "    for i in range(num_seq):\n",
    "        length_seq = np.random.randint(low=low, high=high)\n",
    "        seq_instance = np.random.randint(low=1,high=5, size=(1,length_seq))\n",
    "        raw_seq = \"\".join([list(dna_dict.keys())[x-1] for x in seq_instance[0]])\n",
    "        raw_sequences.append(raw_seq)\n",
    "         \n",
    "    \n",
    "    exp = list(np.random.randn(num_seq))\n",
    "    \n",
    "    dataframe = pd.DataFrame(np.array([raw_sequences,exp])).transpose()\n",
    "    dataframe.columns = ['Seqs','Exp']\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seqs</th>\n",
       "      <th>Exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GTTGTGGTTTC</td>\n",
       "      <td>0.21149332839302568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TTATTGCCCTG</td>\n",
       "      <td>-0.6734173749301339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCTGGCTGACTTTAT</td>\n",
       "      <td>-0.8094650799459052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGACGACCCAA</td>\n",
       "      <td>-0.6704788021178381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GTATATCCAAAT</td>\n",
       "      <td>-0.1483692902697492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GAGAGAGGATCTAAC</td>\n",
       "      <td>-1.4565325911339704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CGTTCAAGCGACGTT</td>\n",
       "      <td>0.05914054769723368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TCGTTATGGGA</td>\n",
       "      <td>-0.29412480775248456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GTTCCTACCCCCGTGA</td>\n",
       "      <td>-0.9986863786450021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TGGCGCCTTTGTGTCCA</td>\n",
       "      <td>-0.768962610264953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AGCTGGTGTATCT</td>\n",
       "      <td>-0.9454579287098425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GCGACCTTAGTGAACT</td>\n",
       "      <td>-2.227856757181979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GAATTCTCACCTAGACT</td>\n",
       "      <td>1.3695793530820117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CGCGAGTTATCTCGGGAAA</td>\n",
       "      <td>0.20557431746062454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TGATGTTATCTT</td>\n",
       "      <td>-2.0033552998355892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CAGAACGCCTTT</td>\n",
       "      <td>0.6119742050493817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TACGTTAGGTGTTGGCTTC</td>\n",
       "      <td>0.8139060037308501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ACTCCTAATAT</td>\n",
       "      <td>0.7094898263361299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TGTCCCTTGAGGGCCATT</td>\n",
       "      <td>-0.0954058200131837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CCAGCCGCGAAC</td>\n",
       "      <td>1.0527303461396535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AAGCATCTGCGT</td>\n",
       "      <td>-1.4965467725090666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CTTTCCGCTCAGAC</td>\n",
       "      <td>-0.37592351855617234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>CGCAGATGACATGCGCAA</td>\n",
       "      <td>-1.6000435922224479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GTTTTCTTTAGGAACTTTA</td>\n",
       "      <td>-0.5052470178831199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AATGCTTGGCAGCGTACAT</td>\n",
       "      <td>0.29750697722067565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>CGATACTGCGGTATA</td>\n",
       "      <td>1.2028735097430845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>TAACCGGGAC</td>\n",
       "      <td>0.014492135583022848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GAGAAATATCCG</td>\n",
       "      <td>0.9037278842092268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ACCCTAGCGG</td>\n",
       "      <td>-0.1589093899695254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>TACGCGAGGCCGCAC</td>\n",
       "      <td>0.6015061666449715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>TGACCCTCTGTGGACTCTA</td>\n",
       "      <td>-1.5580675134456445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>CGAATATGCTA</td>\n",
       "      <td>-0.3962309889853234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>TATGCAGTTCAA</td>\n",
       "      <td>-0.12657319360624378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>GGACACGGCCA</td>\n",
       "      <td>-2.0725649573461573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GAGCAAACTG</td>\n",
       "      <td>1.3552676728259048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AAGGCGAAATGTTTTAA</td>\n",
       "      <td>-1.2006281238088259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>CATGCGCATGAATGTGG</td>\n",
       "      <td>0.9693719265519685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AGGCGCAGACAGT</td>\n",
       "      <td>0.7255667498872482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GAGCCTGGTTCCACC</td>\n",
       "      <td>-0.7268722724466948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>TAAAAAACATAGT</td>\n",
       "      <td>0.8678693782824982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>TGGCACGATC</td>\n",
       "      <td>-0.7419076287094656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>TAGTTGGGAGAGAGACTCT</td>\n",
       "      <td>-0.8122553537797939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AAGTAGGTTCCTG</td>\n",
       "      <td>-0.4990487294158272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>CGACCACTCTGTTCT</td>\n",
       "      <td>-0.19939683398926486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>CTGTTCCTCT</td>\n",
       "      <td>0.001112480053071957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>CGGCTTCCACTTG</td>\n",
       "      <td>-0.9545027659453355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>CATACCTCTCTCA</td>\n",
       "      <td>-0.707770914817555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>GCCCATGACC</td>\n",
       "      <td>-1.015033433132734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>TCGCTGTGTCT</td>\n",
       "      <td>-0.31936709907661714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>TATGAATGTAAACTGG</td>\n",
       "      <td>0.038445879524308474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Seqs                   Exp\n",
       "0           GTTGTGGTTTC   0.21149332839302568\n",
       "1           TTATTGCCCTG   -0.6734173749301339\n",
       "2       CCTGGCTGACTTTAT   -0.8094650799459052\n",
       "3          TCGACGACCCAA   -0.6704788021178381\n",
       "4          GTATATCCAAAT   -0.1483692902697492\n",
       "5       GAGAGAGGATCTAAC   -1.4565325911339704\n",
       "6       CGTTCAAGCGACGTT   0.05914054769723368\n",
       "7           TCGTTATGGGA  -0.29412480775248456\n",
       "8      GTTCCTACCCCCGTGA   -0.9986863786450021\n",
       "9     TGGCGCCTTTGTGTCCA    -0.768962610264953\n",
       "10        AGCTGGTGTATCT   -0.9454579287098425\n",
       "11     GCGACCTTAGTGAACT    -2.227856757181979\n",
       "12    GAATTCTCACCTAGACT    1.3695793530820117\n",
       "13  CGCGAGTTATCTCGGGAAA   0.20557431746062454\n",
       "14         TGATGTTATCTT   -2.0033552998355892\n",
       "15         CAGAACGCCTTT    0.6119742050493817\n",
       "16  TACGTTAGGTGTTGGCTTC    0.8139060037308501\n",
       "17          ACTCCTAATAT    0.7094898263361299\n",
       "18   TGTCCCTTGAGGGCCATT   -0.0954058200131837\n",
       "19         CCAGCCGCGAAC    1.0527303461396535\n",
       "20         AAGCATCTGCGT   -1.4965467725090666\n",
       "21       CTTTCCGCTCAGAC  -0.37592351855617234\n",
       "22   CGCAGATGACATGCGCAA   -1.6000435922224479\n",
       "23  GTTTTCTTTAGGAACTTTA   -0.5052470178831199\n",
       "24  AATGCTTGGCAGCGTACAT   0.29750697722067565\n",
       "25      CGATACTGCGGTATA    1.2028735097430845\n",
       "26           TAACCGGGAC  0.014492135583022848\n",
       "27         GAGAAATATCCG    0.9037278842092268\n",
       "28           ACCCTAGCGG   -0.1589093899695254\n",
       "29      TACGCGAGGCCGCAC    0.6015061666449715\n",
       "30  TGACCCTCTGTGGACTCTA   -1.5580675134456445\n",
       "31          CGAATATGCTA   -0.3962309889853234\n",
       "32         TATGCAGTTCAA  -0.12657319360624378\n",
       "33          GGACACGGCCA   -2.0725649573461573\n",
       "34           GAGCAAACTG    1.3552676728259048\n",
       "35    AAGGCGAAATGTTTTAA   -1.2006281238088259\n",
       "36    CATGCGCATGAATGTGG    0.9693719265519685\n",
       "37        AGGCGCAGACAGT    0.7255667498872482\n",
       "38      GAGCCTGGTTCCACC   -0.7268722724466948\n",
       "39        TAAAAAACATAGT    0.8678693782824982\n",
       "40           TGGCACGATC   -0.7419076287094656\n",
       "41  TAGTTGGGAGAGAGACTCT   -0.8122553537797939\n",
       "42        AAGTAGGTTCCTG   -0.4990487294158272\n",
       "43      CGACCACTCTGTTCT  -0.19939683398926486\n",
       "44           CTGTTCCTCT  0.001112480053071957\n",
       "45        CGGCTTCCACTTG   -0.9545027659453355\n",
       "46        CATACCTCTCTCA    -0.707770914817555\n",
       "47           GCCCATGACC    -1.015033433132734\n",
       "48          TCGCTGTGTCT  -0.31936709907661714\n",
       "49     TATGAATGTAAACTGG  0.038445879524308474"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_Seqs_Exp_Dataset(50,10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3., 1., 1., 1., 3., 2., 1., 1., 1., 3., 3., 2., 2., 2., 1., 2.,\n",
       "       1., 1., 1., 2., 1., 3., 2., 1., 2., 2., 2., 1., 1., 1., 2., 3., 1.,\n",
       "       3., 1., 2., 3., 3., 3., 1., 1., 3., 1., 1., 2., 3., 1., 2., 2., 2.,\n",
       "       2., 1., 2., 2., 2., 1., 1., 3., 1., 2., 2., 2., 2., 1., 1., 2., 2.,\n",
       "       3., 3., 2., 4., 2., 2., 3., 1., 2., 2., 2., 3., 2., 1., 1., 2., 2.,\n",
       "       2., 2., 2., 2., 1., 1., 3., 2., 3., 1., 1., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe todo ; so simulated DNA is from a different ATCG content\n",
    "dist = np.random.poisson(5,100)\n",
    "dist = dist / dist.max()\n",
    "dist = dist * 3\n",
    "dist = dist // 1\n",
    "dist += 1\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
