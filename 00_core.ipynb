{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bionlp\n",
    "\n",
    "> useful fxns for DL models with bio seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bionlp_imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def gen_bio_vocab(frameshift=True, kmer=1, seq='DNA'):\n",
    "    \"\"\" Makes a dictionary for mapping a raw sequence\n",
    "    to one hot encoding\n",
    "    \n",
    "    seq = DNA or PROTEIN\n",
    "    \"\"\"\n",
    "    \n",
    "    if seq == 'DNA':\n",
    "        letters ='ATCGN'\n",
    "    elif seq == 'PROTEIN':\n",
    "        letters = 'ARNDBCEQZGHILKMFPSTWYV'\n",
    "    token_list  = []\n",
    "    if frameshift:\n",
    "        letters = 'X' + letters\n",
    "    for i in [''.join(c) for c in product(letters, repeat=kmer)]:\n",
    "        if (i[0] == 'X') and (i[kmer-1] == 'X'):\n",
    "            pass\n",
    "        else:\n",
    "            token_list.append(i)\n",
    "\n",
    "    mapping = {}\n",
    "    idx = 1\n",
    "    for letter in token_list:\n",
    "        if letter not in mapping:\n",
    "            mapping[letter] = idx\n",
    "            idx+=1        \n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1,\n",
       " 'R': 2,\n",
       " 'N': 3,\n",
       " 'D': 4,\n",
       " 'B': 5,\n",
       " 'C': 6,\n",
       " 'E': 7,\n",
       " 'Q': 8,\n",
       " 'Z': 9,\n",
       " 'G': 10,\n",
       " 'H': 11,\n",
       " 'I': 12,\n",
       " 'L': 13,\n",
       " 'K': 14,\n",
       " 'M': 15,\n",
       " 'F': 16,\n",
       " 'P': 17,\n",
       " 'S': 18,\n",
       " 'T': 19,\n",
       " 'W': 20,\n",
       " 'Y': 21,\n",
       " 'V': 22}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_bio_vocab(seq='PROTEIN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pred_vs_truth(model,dataloader):\n",
    "    \"\"\"\n",
    "    takes a trained model and runs predictions over the\n",
    "    given dataloader.\n",
    "    \n",
    "    plots hist of the ground truths vs predictions\n",
    "    \n",
    "        \"\"\"\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    truths = []\n",
    "    preds =  []\n",
    "    for i in range(len(dataloader)):\n",
    "        batch = next(iter(dataloader))\n",
    "        preds.append(model.predict(batch[0]))\n",
    "        truths.append(batch[1])\n",
    "\n",
    "    array_pred = []\n",
    "    for i in preds:\n",
    "        for x in i:\n",
    "            array_pred.append(x)\n",
    "\n",
    "    flat_preds = np.array(array_pred).flatten()\n",
    "\n",
    "    truth_pred = []\n",
    "    for i in truths:\n",
    "        for x in i:\n",
    "            truth_pred.append(np.array(x))\n",
    "    flat_truth = np.array(truth_pred)\n",
    "\n",
    "\n",
    "    #figures\n",
    "\n",
    "    x_low = flat_truth.min()*-(1.1*flat_truth.min())\n",
    "\n",
    "    figure = plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1, 2, 2)\n",
    "\n",
    "    plt.hist(flat_truth, bins=20, color = 'blue', label='Truths',alpha=.5,\n",
    "             range=(flat_truth.min()*-(1.5*flat_truth.min()),flat_truth.max()*1.5))\n",
    "    plt.hist(flat_preds, bins=20, color = 'red', label='Preds', alpha=.5)\n",
    "    plt.legend(['Truth','Pred'])\n",
    "\n",
    "\n",
    "    r2 = round(np.corrcoef(flat_truth,flat_preds)[0][1],2)\n",
    "\n",
    "    m, b = np. polyfit(flat_truth, flat_preds, 1)# m = slope, b = intercept.\n",
    "\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(flat_truth,flat_preds)\n",
    "    plt.plot(flat_truth, 1*flat_truth, c='blue', linewidth=1 )\n",
    "    plt.plot(flat_truth, m*flat_truth + b, c = 'red', linewidth = 1)\n",
    "    plt.xlabel('truth values')\n",
    "    plt.ylabel('pred values')\n",
    "    plt.text(flat_truth.min(),flat_truth.max()*1.3, f'R2 = {str(r2)} \\n y = {round(m,2)}x + {round(b,2)}', color='red',fontweight='heavy')\n",
    "    plt.xlim(x_low,flat_truth.max()*1.5)\n",
    "    plt.ylim(x_low,flat_truth.max()*1.5)\n",
    "\n",
    "    #plt.show()\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def raw2token(seq,bio_dict):\n",
    "    \"\"\"\n",
    "    takes in raw sequence then converts it to tokenized version\n",
    "    \n",
    "    'ATT' -> [1 2 2]\n",
    "    \"\"\"\n",
    "    #dna_dict = gen_bio_vocab()\n",
    "    return [bio_dict[x] for x in seq]\n",
    "\n",
    "def token2hot(seq, max_length, n_features):\n",
    "    \"\"\"\n",
    "    takes in tokenized sequences and returns 1-hot encoded\n",
    "    \n",
    "    [1 2 2] -> [1 0 0 0], [0 1 0 0 ], [0 1 0 0]\n",
    "    \"\"\"\n",
    "    N = max_length - len(seq)\n",
    "    x = np.pad(seq, (0, N), 'constant')\n",
    "    x = F.one_hot(torch.tensor(x),num_classes=n_features)\n",
    "    return x\n",
    "    \n",
    "def hot2token(seq):\n",
    "    \"\"\" one-hot to tokens\n",
    "    \n",
    "    input:single sequence \n",
    "    \n",
    "    e.g. [[0 0 1 0], [0 0 0 1] , [0 1 0 0]] -> [2 3 1]\n",
    "    \"\"\"\n",
    "    return [np.argmax(i) for i in np.array(seq)]\n",
    "\n",
    "def prep_seqs(seq, max_length=10):\n",
    "    \"\"\"\n",
    "    takes in a raw seq 'ATTATA' -> one hot encodes to specified max_len\n",
    "      \"\"\"\n",
    "    x = raw2token(seq)\n",
    "    x = token2hot(x, max_length)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DnaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    takes in dataframe with ['Seqs'] and ['Exp']\n",
    "    \n",
    "    returns pytorch dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.max_length = self.df.Seqs.map(len).max()\n",
    "        self.df.Exp.astype(float)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        seq = self.df['Seqs'][idx]\n",
    "        seq = prep_seqs(seq,self.max_length)\n",
    "        seq = seq.swapaxes(0,1)\n",
    "        seq = seq.float()\n",
    "\n",
    "        target = self.df['Exp'][idx]\n",
    "        target = torch.tensor(target).float()\n",
    "        #target = torch.tensor(target)\n",
    "\n",
    "        return seq, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "class StepByStep(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "        \n",
    "        # We start by storing the arguments as attributes \n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.writer = None\n",
    "        \n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        #MINE\n",
    "        self.images = []\n",
    "        \n",
    "        # Creates the train_step function for our model, \n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step = self._make_train_step()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step = self._make_val_step()\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def set_tensorboard(self, name, folder='runs'):\n",
    "        # This method allows the user to define a SummaryWriter to interface with TensorBoard\n",
    "        suffix = datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "        self.writer = SummaryWriter('{}/{}_{}'.format(folder, name, suffix))\n",
    "\n",
    "    def _make_train_step(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "        \n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step\n",
    "    \n",
    "    def _make_val_step(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step\n",
    "            \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and \n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step = self.val_step\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step = self.train_step\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False    \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                \n",
    "   \n",
    "                \n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "            # If a SummaryWriter has been set...\n",
    "            if self.writer:\n",
    "                scalars = {'training': loss}\n",
    "                if val_loss is not None:\n",
    "                    scalars.update({'validation': val_loss})\n",
    "                # Records both losses for each epoch under the main tag \"loss\"\n",
    "                self.writer.add_scalars(main_tag='loss',\n",
    "                                        tag_scalar_dict=scalars,\n",
    "                                        global_step=epoch)\n",
    "\n",
    "        if self.writer:\n",
    "            # Closes the writer\n",
    "            self.writer.close()\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training   \n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval() \n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def add_graph(self):\n",
    "        # Fetches a single mini-batch so we can use add_graph\n",
    "        if self.train_loader and self.writer:\n",
    "            x_sample, y_sample = next(iter(self.train_loader))\n",
    "            self.writer.add_graph(self.model, x_sample.to(self.device))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generate_Seqs_Exp_Dataset(num_seq, low, high):\n",
    "    \"\"\"\n",
    "    generates a DNA dataset with Expression(or whatever else) values\n",
    "    nucleotides/expressions will be drawn from 2 distinct distributions\n",
    "    \n",
    "    input : num_seq -> int: number of sequences\n",
    "            length_seq -> tuple(min/max length of sequences)\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_sequences = []\n",
    "    dna_dict = gen_dna_vocab()\n",
    "    \n",
    "    \n",
    "    for i in range(num_seq):\n",
    "        length_seq = np.random.randint(low=low, high=high)\n",
    "        seq_instance = np.random.randint(low=1,high=5, size=(1,length_seq))\n",
    "        raw_seq = \"\".join([list(dna_dict.keys())[x-1] for x in seq_instance[0]])\n",
    "        raw_sequences.append(raw_seq)\n",
    "         \n",
    "    \n",
    "    exp = list(np.random.randn(num_seq))\n",
    "    \n",
    "    dataframe = pd.DataFrame(np.array([raw_sequences,exp])).transpose()\n",
    "    dataframe.columns = ['Seqs','Exp']\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 1., 1., 3., 2., 3., 2., 2., 1., 1., 1., 2., 2., 1., 2., 2.,\n",
       "       2., 2., 2., 1., 3., 2., 2., 2., 2., 1., 1., 2., 1., 2., 1., 2., 3.,\n",
       "       3., 2., 2., 2., 1., 1., 1., 2., 1., 1., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 3., 3., 1., 2., 2., 2., 2., 4., 2., 1., 2., 1., 2., 2., 2., 2.,\n",
       "       1., 2., 2., 1., 1., 2., 2., 2., 1., 1., 3., 1., 1., 2., 2., 2., 2.,\n",
       "       3., 2., 2., 1., 1., 1., 1., 2., 2., 1., 1., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maybe todo ; so simulated DNA is from a different ATCG content\n",
    "dist = np.random.poisson(5,100)\n",
    "dist = dist / dist.max()\n",
    "dist = dist * 3\n",
    "dist = dist // 1\n",
    "dist += 1\n",
    "dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
